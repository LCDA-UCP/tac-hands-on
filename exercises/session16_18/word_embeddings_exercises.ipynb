{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Word Embeddings Exercises\n",
    "\n",
    "In these exercises, we'll practice computing word embeddings and using them for sentiment analysis.\n",
    "\n",
    "- For sentiment analysis, we can go beyond simply counting words.\n",
    "- Instead, we can represent each word numerically with a vector.\n",
    "- This vector can capture syntactic (e.g., parts of speech) and semantic (e.g., meaning) structures of words.\n",
    "\n",
    "We'll explore a classic approach for generating word embeddings, implementing a well-known model called the Continuous Bag of Words (CBOW) model.\n",
    "\n",
    "By completing this assignment, we will:\n",
    "\n",
    "- Train word vectors from scratch.\n",
    "- Understand how backpropagation works.\n",
    "- Plot and visualize the learned word vectors.\n",
    "\n",
    "Training these models will deepen our understanding of word vectors, foundational elements in natural language processing and behavior analysis technologies.\n",
    "\n"
   ],
   "id": "98c7721accefab59"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#  The Continuous Bag of Words (CBOW) Model\n",
    "\n",
    "Consider the following sentence: \n",
    "> **\"I am happy because I am learning.\"**\n",
    "\n",
    "- In the Continuous Bag of Words (CBOW) model, we aim to predict the center word using a few surrounding context words.\n",
    "- For instance, with a context window size of $C = 2$, we would try to predict the word **\"happy\"** using two words before and two words after the center word:\n",
    "\n",
    "> $C$ words before: [I, am]  \n",
    "> $C$ words after: [because, I]  \n",
    "\n",
    "- This gives us:\n",
    "\n",
    "$$\\text{context} = [I, am, because, I]$$  \n",
    "$$\\text{target} = \\text{happy}$$"
   ],
   "id": "5103d6982ff3de39"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "The model structure is shown below:\n",
    "\n",
    "<div style=\"width:image width px; font-size:100%; text-align:center;\"><img src='imgs/word2.png' alt=\"CBOW Model Structure\" width=\"600px\" height=\"250px\" /> Figure 1 </div>\n",
    "\n",
    "In this setup, $\\bar x$ represents the average of the one-hot vectors of the context words.\n",
    "\n",
    "<div style=\"width:image width px; font-size:100%; text-align:center;\"><img src='imgs/mean_vec2.png' alt=\"Mean Vector\" width=\"600px\" height=\"250px\" /> Figure 2 </div>\n",
    "\n",
    "Once all context words are encoded, we use $\\bar x$ as the input to our model.\n",
    "\n",
    "The architecture you will implement is structured as follows:\n",
    "\n",
    "\\begin{align}\n",
    " h &= W_1 \\  X + b_1  \\tag{1} \\\\\n",
    " a &= ReLU(h)  \\tag{2} \\\\\n",
    " z &= W_2 \\  a + b_2   \\tag{3} \\\\\n",
    " \\hat y &= softmax(z)   \\tag{4} \\\\\n",
    "\\end{align}"
   ],
   "id": "db2487b67963bb43"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Exercise 1 - Data Pre-processing\n",
    "\n",
    "- **1.1 Read the file `data/shakespeare.txt`. Use the python `open` function for reading the file.**\n",
    "- **1.2 Remove punctuation and numbers from the text (`,!?;-`).**\n",
    "- **1.3 Tokenize the text using the `nltk` library.**\n",
    "- **1.4 Lowercase the tokens and keep only words with alphabetical characters.**"
   ],
   "id": "74b56f6fe95fbd7e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-14T15:41:02.061613Z",
     "start_time": "2024-11-14T15:41:01.751017Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import re\n",
    "import nltk\n",
    "\n",
    "with open('data/shakespeare.txt', 'r') as f:\n",
    "    data = f.read()\n",
    "    \n",
    "print(data[:200])\n",
    "\n",
    "def preprocess_tokenize(text):\n",
    "    text = re.sub(r'[,!?;-]', '', text)\n",
    "    text = nltk.word_tokenize(text) \n",
    "    return [token.lower() for token in text if token.isalpha()]\n",
    "    \n",
    "data = preprocess_tokenize(data)\n",
    "print(\"Number of tokens:\", len(data),'\\n', data[:15])"
   ],
   "id": "4cfd0f0cdb4c18a9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "O for a Muse of fire, that would ascend\n",
      "The brightest heaven of invention,\n",
      "A kingdom for a stage, princes to act\n",
      "And monarchs to behold the swelling scene!\n",
      "Then should the warlike Harry, like himself,\n",
      "Number of tokens: 51631 \n",
      " ['o', 'for', 'a', 'muse', 'of', 'fire', 'that', 'would', 'ascend', 'the', 'brightest', 'heaven', 'of', 'invention', 'a']\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-14T15:41:04.292185Z",
     "start_time": "2024-11-14T15:41:04.283225Z"
    }
   },
   "cell_type": "code",
   "source": [
    "assert len(data) == 51631, f\"Error in the data length. Expected 51631 but got {len(data)}\"\n",
    "assert data[3] == 'muse', f\"Error: Expected 'muse' but got {data[3]}\"\n",
    "assert data[5] == 'fire', f\"Error: Expected 'fire' but got {data[5]}\""
   ],
   "id": "146fef78d0068478",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Exercise 2 - Word Frequency\n",
    "\n",
    "**2. Print the size of the vocabulary and the 10 most frequent tokens.**"
   ],
   "id": "d6b5fc471535150"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-14T15:42:05.102605Z",
     "start_time": "2024-11-14T15:42:05.073616Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from nltk import FreqDist\n",
    "\n",
    "fdist = FreqDist(data)\n",
    "print(\"Size of vocabulary:\", len(fdist) )\n",
    "print(\"Most frequent tokens:\", fdist.most_common(10))"
   ],
   "id": "3e17a0fa7058fefa",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of vocabulary: 5966\n",
      "Most frequent tokens: [('the', 1521), ('and', 1394), ('i', 1271), ('to', 1159), ('of', 1093), ('my', 857), ('that', 781), ('in', 771), ('a', 753), ('you', 748)]\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-14T15:54:48.681482Z",
     "start_time": "2024-11-14T15:54:48.669021Z"
    }
   },
   "cell_type": "code",
   "source": [
    "assert len(fdist) == 5966, f\"Error: Expected 5841 but got {len(fdist)}\"\n",
    "assert fdist.most_common(1)[0][0] == 'the', f\"Error: The most frequent word should be 'the'\""
   ],
   "id": "a3db34d1893fc8e1",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Exercise 3 - Mapping words to indices and indices to words\n",
    "\n",
    "3. Create two dictionaries:\n",
    "    - `word2Ind`: a dictionary that maps words to unique indices.\n",
    "    - `Ind2word`: a dictionary that maps indices to unique words."
   ],
   "id": "8d2050d43435b99e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-14T15:54:51.303957Z",
     "start_time": "2024-11-14T15:54:51.283206Z"
    }
   },
   "cell_type": "code",
   "source": [
    "word2Ind = {word: value for value, word in enumerate(fdist.keys())}\n",
    "Ind2word = {value: word for word, value in word2Ind.items()}\n",
    "len(word2Ind), len(Ind2word)"
   ],
   "id": "77239a47bee1fb1e",
   "outputs": [
    {
     "data": {
      "text/plain": "(5966, 5966)"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-14T15:54:51.810238Z",
     "start_time": "2024-11-14T15:54:51.800153Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# example of word to index mapping\n",
    "print(\"Index of the word 'king' :  \", word2Ind['king'] )\n",
    "print(\"Word which has index 2743:  \", Ind2word[2743] )"
   ],
   "id": "176d03da09a7e4a2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index of the word 'king' :   173\n",
      "Word which has index 2743:   decree\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-14T15:54:54.205956Z",
     "start_time": "2024-11-14T15:54:54.176016Z"
    }
   },
   "cell_type": "code",
   "source": [
    "assert word2Ind['king'] == 173, f\"Error: The index of the word 'king' should be 128 but got {word2Ind['king']}\"\n",
    "assert Ind2word[2743] == 'decree', f\"Error: The word with index 2743 should be 'decree' but got {Ind2word[2743]}\""
   ],
   "id": "df2c8648daab469a",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Exercise 4 - Sliding Window of Words\n",
    "\n",
    "**4. Implement a function `get_windows(words, C)` that returns a list of tuples, each tuple containing:**\n",
    "- a center word\n",
    "- a list of `2*C` context words"
   ],
   "id": "7a7a6265be2b66e6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-14T16:09:32.691770Z",
     "start_time": "2024-11-14T16:09:32.671071Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def get_windows(words, C):\n",
    "    i = C\n",
    "    while i < len(words) - C:\n",
    "        central_words = words[i]\n",
    "        before_words = words[i-C:i]\n",
    "        after_words = words[i+1:i+C+1]\n",
    "        context_words = before_words + after_words\n",
    "        yield context_words, central_words\n",
    "        i += 1\n",
    "        "
   ],
   "id": "eb8b89707efbe0be",
   "outputs": [],
   "execution_count": 24
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-14T16:09:32.837382Z",
     "start_time": "2024-11-14T16:09:32.826540Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Print 'context_words' and 'center_word' for the new corpus with a 'context half-size' of 2\n",
    "for x, y in get_windows(['i', 'am', 'happy', 'because', 'i', 'am', 'learning'], 2):\n",
    "    print(f'{x}\\t{y}')"
   ],
   "id": "1d28c58a4b6c0f95",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'am', 'because', 'i']\thappy\n",
      "['am', 'happy', 'i', 'am']\tbecause\n",
      "['happy', 'because', 'am', 'learning']\ti\n"
     ]
    }
   ],
   "execution_count": 25
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-14T16:09:33.081215Z",
     "start_time": "2024-11-14T16:09:33.012239Z"
    }
   },
   "cell_type": "code",
   "source": [
    "assert [x for x in get_windows(['i', 'am', 'happy', 'because', 'i', 'am', 'learning'], 2)] == [(['i', 'am', 'because', 'i'], 'happy'), (['am', 'happy', 'i', 'am'], 'because'), (['happy', 'because', 'am', 'learning'], 'i')], \"Error\""
   ],
   "id": "e82eaa22126683df",
   "outputs": [],
   "execution_count": 26
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Exercise 5 - One-hot Encoding\n",
    "\n",
    "**5. Implement a function `word_to_one_hot(word, word2Ind, V)` that gets a word and returns a one-hot encoded vector.**\n",
    "\n",
    "**Note:** \n",
    "- `V` is the size of the vocabulary (the number of words in `word2Ind`).\n",
    "- `word2Ind` is a dictionary that maps words to indices (created previously).\n",
    "- The function should return a 1D numpy array with a length equal to the vocabulary size.\n"
   ],
   "id": "207d1ff13c449964"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-14T16:16:50.941200Z",
     "start_time": "2024-11-14T16:16:50.926352Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "\n",
    "def word_to_one_hot(word, word2Ind, V):\n",
    "    one_hot = np.zeros(V)\n",
    "    one_hot[word2Ind[word]] = 1\n",
    "    return one_hot"
   ],
   "id": "357b36cf240300d7",
   "outputs": [],
   "execution_count": 27
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-14T16:16:51.406342Z",
     "start_time": "2024-11-14T16:16:51.391935Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Example\n",
    "print(word_to_one_hot('muse', word2Ind, len(word2Ind)))\n",
    "print(sum(word_to_one_hot('muse', word2Ind, len(word2Ind))))"
   ],
   "id": "1a49f6ad91875660",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "1.0\n"
     ]
    }
   ],
   "execution_count": 28
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-14T16:16:52.339989Z",
     "start_time": "2024-11-14T16:16:52.311014Z"
    }
   },
   "cell_type": "code",
   "source": [
    "assert sum(word_to_one_hot('muse', word2Ind, len(word2Ind))) == 1, f\"Error: The sum of the one-hot vector should be 1\"\n",
    "assert len(word_to_one_hot('muse', word2Ind, len(word2Ind))) == 5966 == len(word2Ind), f\"Error: The length of the vector should be 5841\""
   ],
   "id": "2a5988c863001efe",
   "outputs": [],
   "execution_count": 29
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Exercise 6 - Context Words to One-hot Encoded Vectors\n",
    "\n",
    "**6. Implement a function `context_words_to_vector(context_words, word2Ind, V)` that returns the average of the one-hot vectors of the words in the context.**\n",
    "\n",
    "**Note:**\n",
    "- `context_words` is a list of words (the context words).\n",
    "- The function should return a 1D numpy array with a length equal to the vocabulary size."
   ],
   "id": "d3b2cc999540a23b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-14T16:22:09.941007Z",
     "start_time": "2024-11-14T16:22:09.924088Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def context_words_to_vector(context_words, word2Ind, V):\n",
    "    one_hot = [word_to_one_hot(word, word2Ind, V) for word in context_words]\n",
    "    return np.mean(one_hot, axis = 0)\n"
   ],
   "id": "cf0d3e9b38923ccc",
   "outputs": [],
   "execution_count": 30
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-14T16:22:10.925706Z",
     "start_time": "2024-11-14T16:22:10.902568Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Example\n",
    "context_words = list(get_windows(data[:10], 2))[0][0]\n",
    "print(context_words_to_vector(context_words, word2Ind, len(word2Ind)))"
   ],
   "id": "663ae6b3c8f46a30",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.25 0.25 0.   ... 0.   0.   0.  ]\n"
     ]
    }
   ],
   "execution_count": 31
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-14T16:24:01.192495Z",
     "start_time": "2024-11-14T16:24:01.160450Z"
    }
   },
   "cell_type": "code",
   "source": [
    "assert sum(context_words_to_vector(context_words, word2Ind, len(word2Ind))) == 1, f\"Error: The sum of the vector should be 1\"\n",
    "assert len(context_words_to_vector(context_words, word2Ind, len(word2Ind))) == 5966 == len(word2Ind), f\"Error: The length of the vector should be 5841\""
   ],
   "id": "1ad12a9e398a9639",
   "outputs": [],
   "execution_count": 32
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Exercise 7 - Building the Training Set\n",
    "\n",
    "**7. Implement a function `get_training_example(words, C, word2Ind, V)` that returns the center word one-hot encoded vector and the average of the one-hot vectors of the context words.**\n",
    "\n",
    "**Note:**\n",
    "- `words` is a list of words.\n",
    "- `C` is the context half-size.\n",
    "- `word2Ind` is a dictionary that maps words to indices.\n",
    "- `V` is the size of the vocabulary."
   ],
   "id": "c5c320d3850fc4fd"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def get_training_example(words, C, word2Ind, V):\n",
    "    # ..."
   ],
   "id": "5217aebd24a8cef4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Example\n",
    "for center_word_one_hot, context_words_avg in get_training_example(data[:10], 2, word2Ind, len(word2Ind)):\n",
    "    print(center_word_one_hot)\n",
    "    print(context_words_avg)"
   ],
   "id": "7a8835f23e607eac",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Continuous Bag of Words Model\n",
    "\n",
    "Here is a more complete diagram of the Continuous Bag of Words (CBOW) model:\n",
    "\n",
    "<div style=\"width:image width px; font-size:100%; text-align:center;\"><img src='imgs/cbow_model_architecture2.png' alt=\"CBOW Model Structure\" width=\"600px\" height=\"250px\" /> Figure 3 </div>"
   ],
   "id": "f44907eec15c9f15"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Let's start by the activation functions:\n",
    "\n",
    "- **ReLU:** The rectified linear activation function is given by:\n",
    "$$ ReLU(x) = max(0, x) $$\n",
    "- **Softmax:** The softmax function is a generalization of the logistic function that \"squashes\" a K-dimensional vector z of arbitrary real values to a K-dimensional vector $\\sigma(z)$ of real values in the range (0, 1) that add up to 1. The function is given by:\n",
    "$$ \\sigma(z)_j = \\frac{e^{z_j}}{\\sum_{k=1}^{K} e^{z_k}} $$"
   ],
   "id": "3429eaf63acb89dd"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Exercise 8 - Activation Functions\n",
    "\n",
    "**8.1 Implement the ReLU activation function.**\n",
    "- ReLu is used to calculate the values of the hidden layer, in the following way:\n",
    "$$ z_1 = W_1 \\  X + b_1 $$\n",
    "$$ h = ReLU(z_1) $$\n",
    "\n",
    "**8.2 Implement the softmax activation function.**\n",
    "- Softmax is used to calculate the values of the output layer, in the following way:\n",
    "$$ z_2 = W_2 \\  a + b_2 $$\n",
    "$$ \\hat y = softmax(z_2) $$"
   ],
   "id": "96770e2af4c465bf"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def ReLU(x):\n",
    "    return # ..."
   ],
   "id": "e09cd3bf898ff463",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Example\n",
    "print(ReLU(np.array([1, -1, 0])))"
   ],
   "id": "36308ef3bfa72d15",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "assert np.all(ReLU(np.array([1, -1, 0])) == np.array([1, 0, 0])), f\"Error: The ReLU function is not correct!\""
   ],
   "id": "ef455aa9c45ac330",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def softmax(z):\n",
    "    return # ..."
   ],
   "id": "3899d90b72da2ce",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Example\n",
    "print(softmax(np.array([1, 2, 3])))"
   ],
   "id": "7bfab27f0b27620d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "assert np.allclose(softmax(np.array([1, 2, 3])), np.array([0.09003057, 0.24472847, 0.66524096])), f\"Error: The softmax function is not correct!\"\n",
    "assert np.allclose(np.sum(softmax(np.array([1, 2, 3]))), 1), f\"Error: The sum of the softmax function should be 1\""
   ],
   "id": "538ad7194c114b1e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Trainig the CBOW Model\n",
    "\n",
    "<img src=\"imgs/forward.png\">\n",
    "\n",
    "Now that we have implemented the necessary functions to prepare the training data and the activation functions, we can start training the CBOW model.\n",
    "\n",
    "The model will be trained using the following steps:\n",
    "1. Initialize the weights and biases.\n",
    "2. Iterate over the training examples and prepare the input and target output.\n",
    "3. Calculate the predicted output using the forward pass.\n",
    "4. Calculate the loss using the predicted output and the target output.\n",
    "5. Update the weights and biases using backpropagation.\n",
    "6. Repeat the process for a number of epochs.\n"
   ],
   "id": "40d5d5f821381013"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Exercise 9 - Initialize the weights and biases\n",
    "\n",
    "**9.1 Initialize the weights `W1` and `W2` with random values from a normal distribution using `np.random.randn` with a seed 0.**\n",
    "- `W1` should be of shape `(N, V)`.\n",
    "- `W2` should be of shape `(V, N)`.\n",
    "- `N` is the dimension of the word embedding.\n",
    "- `V` is the size of the vocabulary.\n",
    "\n",
    "**9.2 Initialize the biases `b1` and `b2` with zeros.**\n",
    "- `b1` should be of shape `(N, 1)`.\n",
    "- `b2` should be of shape `(V, 1)`.\n",
    "\n",
    "**Note:** Use the same function to initialize the weights and biases. Also, set the seed to 42.\n"
   ],
   "id": "da5d50b43257cf79"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def initialize_model_parameters(V, N, random_seed=42):\n",
    "    np.random.seed(random_seed)\n",
    "    # ...\n",
    "    return W1, W2, b1, b2"
   ],
   "id": "457566eae93bc83",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Example\n",
    "V = len(word2Ind)\n",
    "N = 10\n",
    "W1, W2, b1, b2 = initialize_model_parameters(V, N)\n",
    "print(W1.shape, W2.shape, b1.shape, b2.shape)"
   ],
   "id": "9881d2018ea4701b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "assert W1.shape == (10, 5966), f\"Error: The shape of W1 should be (5841, 10) but got {W1.shape}\"\n",
    "assert W2.shape == (5966, 10), f\"Error: The shape of W2 should be (10, 5841) but got {W2.shape}\"\n",
    "assert b1.shape == (10, 1), f\"Error: The shape of b1 should be (10, 1) but got {b1.shape}\"\n",
    "assert b2.shape == (5966, 1), f\"Error: The shape of b2 should be (5841, 1) but got {b2.shape}\""
   ],
   "id": "b14524b5b558c90a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Exercise 10 - Loss Function (Cross-Entropy)\n",
    "\n",
    "**10. Implement the cross-entropy loss function.**\n",
    "- The loss function shoud receive the predicted output `y_hat` and the target output `y`.\n",
    "- The loss function, cross-entropy, is given by:\n",
    "$$ J = - \\sum_{i=1}^{V} y_i \\log(\\hat y_i) $$\n",
    "\n",
    "Where:\n",
    "- $V$ is the size of the vocabulary.\n",
    "- $y$ is the one-hot encoded target word.\n",
    "- $\\hat y$ is the predicted output.\n"
   ],
   "id": "48b47f5340b3e922"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def cross_entropy_loss(y, y_hat):\n",
    "    return # ..."
   ],
   "id": "51a1d5f1a7d2530b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Example\n",
    "y = np.array([1, 0, 0])\n",
    "y_hat = np.array([0.7, 0.2, 0.1])\n",
    "print(cross_entropy_loss(y, y_hat))"
   ],
   "id": "b646a044fdc8575a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "assert np.allclose(cross_entropy_loss(y, y_hat), 0.35667494393873245), f\"Error: The loss is not correct!\""
   ],
   "id": "c20f9f4bbe4b24f8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Exercise 11 - Forward Pass\n",
    "\n",
    "**11. Implement the forward pass of the CBOW model.**\n",
    "- The forward pass is given by:\n",
    "$$ z_1 = W_1 \\  X + b_1 $$\n",
    "$$ h = ReLU(z_1) $$\n",
    "$$ z_2 = W_2 \\  h + b_2 $$\n",
    "$$ \\hat y = softmax(z_2) $$\n",
    "\n",
    "- The function should return the predicted output $\\hat y$.\n",
    "- The function should also return the hidden layer output $h$.\n",
    "- The input $X$ is the average of the one-hot vectors of the context words.\n",
    "- The target output $y$ is the one-hot encoded vector of the center word.\n",
    "- The weights $W_1$ and $W_2$, and the biases $b_1$ and $b_2$ are the parameters of the model."
   ],
   "id": "d269803ac1e945b9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def forward_pass(X, W1, W2, b1, b2):\n",
    "    # ...\n",
    "    return y_hat, h"
   ],
   "id": "4f022cd901e691ae",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Example\n",
    "training_examples = get_training_example(data, 2, word2Ind, V)\n",
    "x, y = next(training_examples)"
   ],
   "id": "4561c0e1d3a2ed18",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "x.shape = (V, 1)\n",
    "y.shape = (V, 1)\n",
    "y_hat, h = forward_pass(x, W1, W2, b1, b2)\n",
    "print(y_hat)"
   ],
   "id": "187c555721396641",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "assert y_hat.shape == (5966, 1), f\"Error: The shape of y_hat should be (5966, 1) but got {y_hat.shape}\""
   ],
   "id": "852f2f7e81d48eb3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Exercise 12 - Backward Pass\n",
    "\n",
    "**12. Implement the backward pass of the CBOW model.**\n",
    "- The backward pass is used to calculate the gradients of the model parameters.\n",
    "- The gradients are used to update the weights and biases during training.\n",
    "- The function should receive the input $X$, the target output $y$, the predicted output $\\hat y$, the hidden layer output $h$, and the model parameters $W1$, $W2$, $b1$, and $b2$ and finally the learning rate $\\alpha$.\n",
    "- The function should return the updated weights and biases.\n",
    "\n",
    "The gradients are calculated as follows:\n",
    "$$ \\frac{\\partial J}{\\partial z_2} = \\hat y - y $$\n",
    "$$ \\frac{\\partial J}{\\partial W_2} = \\frac{\\partial J}{\\partial z_2} \\cdot h^T $$\n",
    "$$ \\frac{\\partial J}{\\partial b_2} = \\frac{\\partial J}{\\partial z_2} $$\n",
    "$$ \\frac{\\partial J}{\\partial z_1} = W_2^T \\cdot \\frac{\\partial J}{\\partial z_2} \\cdot ReLU'(z_1) $$\n",
    "$$ \\frac{\\partial J}{\\partial W_1} = \\frac{\\partial J}{\\partial z_1} \\cdot X^T $$\n",
    "$$ \\frac{\\partial J}{\\partial b_1} = \\frac{\\partial J}{\\partial z_1} $$\n",
    "\n",
    "The weight and bias updates are calculated using the gradients and the learning rate $\\alpha$:\n",
    "\n",
    "$$ W_1 = W_1 - \\alpha \\cdot \\frac{\\partial J}{\\partial W_1} $$\n",
    "$$ W_2 = W_2 - \\alpha \\cdot \\frac{\\partial J}{\\partial W_2} $$\n",
    "$$ b_1 = b_1 - \\alpha \\cdot \\frac{\\partial J}{\\partial b_1} $$\n",
    "$$ b_2 = b_2 - \\alpha \\cdot \\frac{\\partial J}{\\partial b_2} $$\n"
   ],
   "id": "d9965d0e947d9cf4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def backward_pass(X, y, y_hat, h, W1, W2, b1, b2, alpha=0.03):\n",
    "    # ...\n",
    "    return W1, W2, b1, b2\n",
    "    "
   ],
   "id": "27ce7683870a7fd7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Example\n",
    "V = len(word2Ind)\n",
    "N = 10\n",
    "W1, W2, b1, b2 = initialize_model_parameters(V, N)\n",
    "x, y = next(get_training_example(data, 2, word2Ind, V))\n",
    "x.shape = (V, 1)\n",
    "y.shape = (V, 1)\n",
    "y_hat, h = forward_pass(x, W1, W2, b1, b2)\n",
    "W1, W2, b1, b2 = backward_pass(x, y, y_hat, h, W1, W2, b1, b2)\n",
    "print(W1.shape, W2.shape, b1.shape, b2.shape)"
   ],
   "id": "bed6520d82043a78",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "assert W1.shape == (N, V), f\"Error: The shape of W1 should be ({N}, {V}) but got {W1.shape}\"\n",
    "assert W2.shape == (V, N), f\"Error: The shape of W2 should be ({V}, {N}) but got {W2.shape}\"\n",
    "assert b1.shape == (N, 1), f\"Error: The shape of b1 should be ({N}, 1) but got {b1.shape}\"\n",
    "assert b2.shape == (V, 1), f\"Error: The shape of b2 should be ({V}, 1) but got {b2.shape}\""
   ],
   "id": "145e352e5d9f1992",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Exercise 13 - Training the CBOW Model with Gradient Descent\n",
    "\n",
    "**13. Implement the training loop of the CBOW model.**\n",
    "- The training loop should iterate over the training examples and update the weights and biases using the backward pass.\n",
    "- Compute the loss at each epoch and print it.\n",
    "- The function should return the final weights and biases.\n"
   ],
   "id": "99972f53fba92c7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def gradient_descent(data, word2Ind, N, alpha, epochs, seed=42):\n",
    "    # ...\n",
    "    return W1, W2, b1, b2"
   ],
   "id": "c89eb38dcd0fcb89",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Example\n",
    "N = 10\n",
    "alpha = 0.03\n",
    "epochs = 5\n",
    "W1, W2, b1, b2 = gradient_descent(data, word2Ind, N, alpha, epochs)"
   ],
   "id": "a716941ec121c595",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Exercise 14 - Extracting the Word Embeddings\n",
    "\n",
    "**14. Implement three versions of the function `extract_word_embedding(W1, word, word2Ind)` that returns the word embedding of a given word.**\n",
    "\n",
    "- 14.1 Extract the embeddings from the weights $W_1$.\n",
    "- 14.2 Extract the embedding from the weights $W_2$.\n",
    "- 14.3 Return the average of the two embeddings."
   ],
   "id": "7cc3fa2ab82dfa20"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def extract_word_embedding_w1(W1, word, word2Ind):\n",
    "    return # ..."
   ],
   "id": "ce987fcff2721e66",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def extract_word_embedding_w2(W2, word, word2Ind):\n",
    "    return # ..."
   ],
   "id": "a40523ac60093e30",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def extract_word_embedding_avg(W1, W2, word, word2Ind):\n",
    "    return # ..."
   ],
   "id": "b278955748c530af",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Example\n",
    "word = 'king'\n",
    "embedding_w1 = extract_word_embedding_w1(W1, word, word2Ind)\n",
    "embedding_w2 = extract_word_embedding_w2(W2, word, word2Ind)\n",
    "embedding_avg = extract_word_embedding_avg(W1, W2, word, word2Ind)\n",
    "embedding_w1"
   ],
   "id": "5445549c60f4f5e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "embedding_w2"
   ],
   "id": "82d1f8dd7652696a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "embedding_avg"
   ],
   "id": "43a453578fc18ff6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "assert embedding_w1.shape == (N,), f\"Error: The shape of the embedding should be ({N},) but got {embedding_w1.shape}\"\n",
    "assert embedding_w2.shape == (N,), f\"Error: The shape of the embedding should be ({N},) but got {embedding_w2.shape}\"\n",
    "assert embedding_avg.shape == (N,), f\"Error: The shape of the embedding should be ({N},) but got {embedding_avg.shape}\""
   ],
   "id": "81fc8adfdb88c754",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Exercise 15 - Visualizing the Word Embeddings\n",
    "\n",
    "**15. Implement a function `plot_word_embeddings(W, word2Ind, words)` that plots the word embeddings of the given words.**\n",
    "\n",
    "**Note:**\n",
    "- Use the PCA algorithm to reduce the dimensionality of the embeddings to 2D.\n",
    "- Use the `scatter` function from `matplotlib.pyplot` to plot the embeddings."
   ],
   "id": "46bf70c30fa8b7b4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_word_embeddings(W, word2Ind, words):\n",
    "    # ..."
   ],
   "id": "a22d7be3cd3572e2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Example\n",
    "words = ['king', 'queen', 'man', 'woman']\n",
    "plot_word_embeddings(W1, word2Ind, words)"
   ],
   "id": "476e153529904a9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "plot_word_embeddings(W2.T, word2Ind, words)"
   ],
   "id": "3a9842757114ca48",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "plot_word_embeddings((W1 + W2.T) / 2, word2Ind, words)"
   ],
   "id": "5b61950a3ff20886",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [],
   "id": "18cd8f38c94f8c70"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
