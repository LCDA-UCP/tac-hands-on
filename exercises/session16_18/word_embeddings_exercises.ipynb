{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Word Embeddings Exercises\n",
    "\n",
    "In these exercises, we'll practice computing word embeddings and using them for sentiment analysis.\n",
    "\n",
    "- For sentiment analysis, we can go beyond simply counting words.\n",
    "- Instead, we can represent each word numerically with a vector.\n",
    "- This vector can capture syntactic (e.g., parts of speech) and semantic (e.g., meaning) structures of words.\n",
    "\n",
    "We'll explore a classic approach for generating word embeddings, implementing a well-known model called the Continuous Bag of Words (CBOW) model.\n",
    "\n",
    "By completing this assignment, we will:\n",
    "\n",
    "- Train word vectors from scratch.\n",
    "- Understand how backpropagation works.\n",
    "- Plot and visualize the learned word vectors.\n",
    "\n",
    "Training these models will deepen our understanding of word vectors, foundational elements in natural language processing and behavior analysis technologies.\n",
    "\n"
   ],
   "id": "98c7721accefab59"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#  The Continuous Bag of Words (CBOW) Model\n",
    "\n",
    "Consider the following sentence: \n",
    "> **\"I am happy because I am learning.\"**\n",
    "\n",
    "- In the Continuous Bag of Words (CBOW) model, we aim to predict the center word using a few surrounding context words.\n",
    "- For instance, with a context window size of $C = 2$, we would try to predict the word **\"happy\"** using two words before and two words after the center word:\n",
    "\n",
    "> $C$ words before: [I, am]  \n",
    "> $C$ words after: [because, I]  \n",
    "\n",
    "- This gives us:\n",
    "\n",
    "$$\\text{context} = [I, am, because, I]$$  \n",
    "$$\\text{target} = \\text{happy}$$"
   ],
   "id": "5103d6982ff3de39"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "The model structure is shown below:\n",
    "\n",
    "<div style=\"width:image width px; font-size:100%; text-align:center;\"><img src='imgs/word2.png' alt=\"CBOW Model Structure\" width=\"600px\" height=\"250px\" /> Figure 1 </div>\n",
    "\n",
    "In this setup, $\\bar x$ represents the average of the one-hot vectors of the context words.\n",
    "\n",
    "<div style=\"width:image width px; font-size:100%; text-align:center;\"><img src='imgs/mean_vec2.png' alt=\"Mean Vector\" width=\"600px\" height=\"250px\" /> Figure 2 </div>\n",
    "\n",
    "Once all context words are encoded, we use $\\bar x$ as the input to our model.\n",
    "\n",
    "The architecture you will implement is structured as follows:\n",
    "\n",
    "\\begin{align}\n",
    " h &= W_1 \\  X + b_1  \\tag{1} \\\\\n",
    " a &= ReLU(h)  \\tag{2} \\\\\n",
    " z &= W_2 \\  a + b_2   \\tag{3} \\\\\n",
    " \\hat y &= softmax(z)   \\tag{4} \\\\\n",
    "\\end{align}"
   ],
   "id": "db2487b67963bb43"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Exercise 1 - Data Pre-processing\n",
    "\n",
    "- **1.1 Read the file `data/shakespeare.txt`. Use the python `open` function for reading the file.**\n",
    "- **1.2 Remove punctuation and numbers from the text (`,!?;-`).**\n",
    "- **1.3 Tokenize the text using the `nltk` library.**\n",
    "- **1.4 Lowercase the tokens and keep only words with alphabetical characters.**"
   ],
   "id": "74b56f6fe95fbd7e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-21T15:05:03.820849Z",
     "start_time": "2024-11-21T15:05:00.790939Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import re\n",
    "import nltk\n",
    "\n",
    "with open('data/shakespeare.txt', 'r') as f:\n",
    "    data = f.read()\n",
    "    \n",
    "print(data[:200])\n",
    "\n",
    "def preprocess_tokenize(text):\n",
    "    text = re.sub(r'[,!?;-]', '', text)\n",
    "    text = nltk.word_tokenize(text) \n",
    "    return [token.lower() for token in text if token.isalpha()]\n",
    "    \n",
    "data = preprocess_tokenize(data)\n",
    "print(\"Number of tokens:\", len(data),'\\n', data[:15])"
   ],
   "id": "4cfd0f0cdb4c18a9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "O for a Muse of fire, that would ascend\n",
      "The brightest heaven of invention,\n",
      "A kingdom for a stage, princes to act\n",
      "And monarchs to behold the swelling scene!\n",
      "Then should the warlike Harry, like himself,\n",
      "Number of tokens: 51631 \n",
      " ['o', 'for', 'a', 'muse', 'of', 'fire', 'that', 'would', 'ascend', 'the', 'brightest', 'heaven', 'of', 'invention', 'a']\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-21T15:05:03.825248Z",
     "start_time": "2024-11-21T15:05:03.820213Z"
    }
   },
   "cell_type": "code",
   "source": [
    "assert len(data) == 51631, f\"Error in the data length. Expected 51631 but got {len(data)}\"\n",
    "assert data[3] == 'muse', f\"Error: Expected 'muse' but got {data[3]}\"\n",
    "assert data[5] == 'fire', f\"Error: Expected 'fire' but got {data[5]}\""
   ],
   "id": "146fef78d0068478",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Exercise 2 - Word Frequency\n",
    "\n",
    "**2. Print the size of the vocabulary and the 10 most frequent tokens.**"
   ],
   "id": "d6b5fc471535150"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-21T15:05:03.853239Z",
     "start_time": "2024-11-21T15:05:03.848877Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from nltk import FreqDist\n",
    "\n",
    "fdist = FreqDist(data)\n",
    "print(\"Size of vocabulary:\", len(fdist) )\n",
    "print(\"Most frequent tokens:\", fdist.most_common(10))"
   ],
   "id": "3e17a0fa7058fefa",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of vocabulary: 5966\n",
      "Most frequent tokens: [('the', 1521), ('and', 1394), ('i', 1271), ('to', 1159), ('of', 1093), ('my', 857), ('that', 781), ('in', 771), ('a', 753), ('you', 748)]\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-21T15:05:03.870387Z",
     "start_time": "2024-11-21T15:05:03.854761Z"
    }
   },
   "cell_type": "code",
   "source": [
    "assert len(fdist) == 5966, f\"Error: Expected 5841 but got {len(fdist)}\"\n",
    "assert fdist.most_common(1)[0][0] == 'the', f\"Error: The most frequent word should be 'the'\""
   ],
   "id": "a3db34d1893fc8e1",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Exercise 3 - Mapping words to indices and indices to words\n",
    "\n",
    "3. Create two dictionaries:\n",
    "    - `word2Ind`: a dictionary that maps words to unique indices.\n",
    "    - `Ind2word`: a dictionary that maps indices to unique words."
   ],
   "id": "8d2050d43435b99e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-21T15:05:03.883521Z",
     "start_time": "2024-11-21T15:05:03.861316Z"
    }
   },
   "cell_type": "code",
   "source": [
    "word2Ind = {word: value for value, word in enumerate(fdist.keys())}\n",
    "Ind2word = {value: word for word, value in word2Ind.items()}\n",
    "len(word2Ind), len(Ind2word)"
   ],
   "id": "77239a47bee1fb1e",
   "outputs": [
    {
     "data": {
      "text/plain": "(5966, 5966)"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-21T15:05:03.885886Z",
     "start_time": "2024-11-21T15:05:03.865749Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# example of word to index mapping\n",
    "print(\"Index of the word 'king' :  \", word2Ind['king'] )\n",
    "print(\"Word which has index 2743:  \", Ind2word[2743] )"
   ],
   "id": "176d03da09a7e4a2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index of the word 'king' :   173\n",
      "Word which has index 2743:   decree\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-21T15:05:03.951123Z",
     "start_time": "2024-11-21T15:05:03.869949Z"
    }
   },
   "cell_type": "code",
   "source": [
    "assert word2Ind['king'] == 173, f\"Error: The index of the word 'king' should be 128 but got {word2Ind['king']}\"\n",
    "assert Ind2word[2743] == 'decree', f\"Error: The word with index 2743 should be 'decree' but got {Ind2word[2743]}\""
   ],
   "id": "df2c8648daab469a",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Exercise 4 - Sliding Window of Words\n",
    "\n",
    "**4. Implement a function `get_windows(words, C)` that returns a list of tuples, each tuple containing:**\n",
    "- a center word\n",
    "- a list of `2*C` context words"
   ],
   "id": "7a7a6265be2b66e6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-21T15:05:03.969723Z",
     "start_time": "2024-11-21T15:05:03.874048Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def get_windows(words, C):\n",
    "    i = C\n",
    "    while i < len(words) - C:\n",
    "        central_words = words[i]\n",
    "        before_words = words[i-C:i]\n",
    "        after_words = words[i+1:i+C+1]\n",
    "        context_words = before_words + after_words\n",
    "        yield context_words, central_words\n",
    "        i += 1\n",
    "        "
   ],
   "id": "eb8b89707efbe0be",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-21T15:05:03.972371Z",
     "start_time": "2024-11-21T15:05:03.877637Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Print 'context_words' and 'center_word' for the new corpus with a 'context half-size' of 2\n",
    "for x, y in get_windows(['i', 'am', 'happy', 'because', 'i', 'am', 'learning'], 2):\n",
    "    print(f'{x}\\t{y}')"
   ],
   "id": "1d28c58a4b6c0f95",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'am', 'because', 'i']\thappy\n",
      "['am', 'happy', 'i', 'am']\tbecause\n",
      "['happy', 'because', 'am', 'learning']\ti\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-21T15:05:04.003708Z",
     "start_time": "2024-11-21T15:05:03.884538Z"
    }
   },
   "cell_type": "code",
   "source": [
    "assert [x for x in get_windows(['i', 'am', 'happy', 'because', 'i', 'am', 'learning'], 2)] == [(['i', 'am', 'because', 'i'], 'happy'), (['am', 'happy', 'i', 'am'], 'because'), (['happy', 'because', 'am', 'learning'], 'i')], \"Error\""
   ],
   "id": "e82eaa22126683df",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Exercise 5 - One-hot Encoding\n",
    "\n",
    "**5. Implement a function `word_to_one_hot(word, word2Ind, V)` that gets a word and returns a one-hot encoded vector.**\n",
    "\n",
    "**Note:** \n",
    "- `V` is the size of the vocabulary (the number of words in `word2Ind`).\n",
    "- `word2Ind` is a dictionary that maps words to indices (created previously).\n",
    "- The function should return a 1D numpy array with a length equal to the vocabulary size.\n"
   ],
   "id": "207d1ff13c449964"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-21T15:05:04.006314Z",
     "start_time": "2024-11-21T15:05:03.887556Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "\n",
    "def word_to_one_hot(word, word2Ind, V):\n",
    "    one_hot = np.zeros(V)\n",
    "    one_hot[word2Ind[word]] = 1\n",
    "    return one_hot"
   ],
   "id": "357b36cf240300d7",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-21T15:05:04.018043Z",
     "start_time": "2024-11-21T15:05:03.891460Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Example\n",
    "print(word_to_one_hot('muse', word2Ind, len(word2Ind)))\n",
    "print(sum(word_to_one_hot('muse', word2Ind, len(word2Ind))))"
   ],
   "id": "1a49f6ad91875660",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "1.0\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-21T15:05:04.021353Z",
     "start_time": "2024-11-21T15:05:03.896016Z"
    }
   },
   "cell_type": "code",
   "source": [
    "assert sum(word_to_one_hot('muse', word2Ind, len(word2Ind))) == 1, f\"Error: The sum of the one-hot vector should be 1\"\n",
    "assert len(word_to_one_hot('muse', word2Ind, len(word2Ind))) == 5966 == len(word2Ind), f\"Error: The length of the vector should be 5841\""
   ],
   "id": "2a5988c863001efe",
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Exercise 6 - Context Words to One-hot Encoded Vectors\n",
    "\n",
    "**6. Implement a function `context_words_to_vector(context_words, word2Ind, V)` that returns the average of the one-hot vectors of the words in the context.**\n",
    "\n",
    "**Note:**\n",
    "- `context_words` is a list of words (the context words).\n",
    "- The function should return a 1D numpy array with a length equal to the vocabulary size."
   ],
   "id": "d3b2cc999540a23b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-21T15:05:04.024399Z",
     "start_time": "2024-11-21T15:05:03.899637Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def context_words_to_vector(context_words, word2Ind, V):\n",
    "    one_hot = [word_to_one_hot(word, word2Ind, V) for word in context_words]\n",
    "    return np.mean(one_hot, axis = 0)\n"
   ],
   "id": "cf0d3e9b38923ccc",
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-21T15:05:04.027346Z",
     "start_time": "2024-11-21T15:05:03.902810Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Example\n",
    "context_words = list(get_windows(data[:10], 2))[0][0]\n",
    "print(context_words_to_vector(context_words, word2Ind, len(word2Ind)))"
   ],
   "id": "663ae6b3c8f46a30",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.25 0.25 0.   ... 0.   0.   0.  ]\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-21T15:05:04.138444Z",
     "start_time": "2024-11-21T15:05:03.907423Z"
    }
   },
   "cell_type": "code",
   "source": [
    "assert sum(context_words_to_vector(context_words, word2Ind, len(word2Ind))) == 1, f\"Error: The sum of the vector should be 1\"\n",
    "assert len(context_words_to_vector(context_words, word2Ind, len(word2Ind))) == 5966 == len(word2Ind), f\"Error: The length of the vector should be 5841\""
   ],
   "id": "1ad12a9e398a9639",
   "outputs": [],
   "execution_count": 18
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Exercise 7 - Building the Training Set\n",
    "\n",
    "**7. Implement a function `get_training_example(words, C, word2Ind, V)` that returns the center word one-hot encoded vector and the average of the one-hot vectors of the context words.**\n",
    "\n",
    "**Note:**\n",
    "- `words` is a list of words.\n",
    "- `C` is the context half-size.\n",
    "- `word2Ind` is a dictionary that maps words to indices.\n",
    "- `V` is the size of the vocabulary."
   ],
   "id": "c5c320d3850fc4fd"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-21T15:05:04.141135Z",
     "start_time": "2024-11-21T15:05:03.912629Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def get_training_example(words, C, word2Ind, V):\n",
    "    for context_word, central_word in get_windows(words, C):\n",
    "        yield word_to_one_hot(central_word, word2Ind, V), context_words_to_vector(context_word, word2Ind, V)"
   ],
   "id": "5217aebd24a8cef4",
   "outputs": [],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-21T15:05:04.143687Z",
     "start_time": "2024-11-21T15:05:03.916905Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Example\n",
    "for center_word_one_hot, context_words_avg in get_training_example(data[:10], 2, word2Ind, len(word2Ind)):\n",
    "    print(center_word_one_hot)\n",
    "    print(context_words_avg)"
   ],
   "id": "7a8835f23e607eac",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 1. ... 0. 0. 0.]\n",
      "[0.25 0.25 0.   ... 0.   0.   0.  ]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0.   0.25 0.25 ... 0.   0.   0.  ]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0.   0.   0.25 ... 0.   0.   0.  ]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Continuous Bag of Words Model\n",
    "\n",
    "Here is a more complete diagram of the Continuous Bag of Words (CBOW) model:\n",
    "\n",
    "<div style=\"width:image width px; font-size:100%; text-align:center;\"><img src='imgs/cbow_model_architecture2.png' alt=\"CBOW Model Structure\" width=\"600px\" height=\"250px\" /> Figure 3 </div>"
   ],
   "id": "f44907eec15c9f15"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Let's start by the activation functions:\n",
    "\n",
    "- **ReLU:** The rectified linear activation function is given by:\n",
    "$$ ReLU(x) = max(0, x) $$\n",
    "- **Softmax:** The softmax function is a generalization of the logistic function that \"squashes\" a K-dimensional vector z of arbitrary real values to a K-dimensional vector $\\sigma(z)$ of real values in the range (0, 1) that add up to 1. The function is given by:\n",
    "$$ \\sigma(z)_j = \\frac{e^{z_j}}{\\sum_{k=1}^{K} e^{z_k}} $$"
   ],
   "id": "3429eaf63acb89dd"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Exercise 8 - Activation Functions\n",
    "\n",
    "**8.1 Implement the ReLU activation function.**\n",
    "- ReLu is used to calculate the values of the hidden layer, in the following way:\n",
    "$$ z_1 = W_1 \\  X + b_1 $$\n",
    "$$ h = ReLU(z_1) $$\n",
    "\n",
    "**8.2 Implement the softmax activation function.**\n",
    "- Softmax is used to calculate the values of the output layer, in the following way:\n",
    "$$ z_2 = W_2 \\  a + b_2 $$\n",
    "$$ \\hat y = softmax(z_2) $$"
   ],
   "id": "96770e2af4c465bf"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-21T15:14:42.044614Z",
     "start_time": "2024-11-21T15:14:42.030318Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def ReLU(x):\n",
    "    return np.maximum(0, x)"
   ],
   "id": "e09cd3bf898ff463",
   "outputs": [],
   "execution_count": 24
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-21T15:14:42.565737Z",
     "start_time": "2024-11-21T15:14:42.557845Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Example\n",
    "print(ReLU(np.array([1, -1, 0])))"
   ],
   "id": "36308ef3bfa72d15",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0 0]\n"
     ]
    }
   ],
   "execution_count": 25
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-21T15:15:20.416648Z",
     "start_time": "2024-11-21T15:15:20.400468Z"
    }
   },
   "cell_type": "code",
   "source": [
    "assert np.all(ReLU(np.array([1, -1, 0])) == np.array([1, 0, 0])), f\"Error: The ReLU function is not correct!\""
   ],
   "id": "ef455aa9c45ac330",
   "outputs": [],
   "execution_count": 26
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-21T15:17:33.767178Z",
     "start_time": "2024-11-21T15:17:33.755438Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def softmax(z):\n",
    "    return np.exp(z) / np.sum(np.exp(z))"
   ],
   "id": "3899d90b72da2ce",
   "outputs": [],
   "execution_count": 27
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-21T15:17:41.778682Z",
     "start_time": "2024-11-21T15:17:41.766599Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(softmax(np.array([1, 2, 3])))"
   ],
   "id": "7bfab27f0b27620d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.09003057 0.24472847 0.66524096]\n"
     ]
    }
   ],
   "execution_count": 28
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-21T15:18:16.060953Z",
     "start_time": "2024-11-21T15:18:16.031499Z"
    }
   },
   "cell_type": "code",
   "source": [
    "assert np.allclose(softmax(np.array([1, 2, 3])), np.array([0.09003057, 0.24472847, 0.66524096])), f\"Error: The softmax function is not correct!\"\n",
    "assert np.allclose(np.sum(softmax(np.array([1, 2, 3]))), 1), f\"Error: The sum of the softmax function should be 1\""
   ],
   "id": "538ad7194c114b1e",
   "outputs": [],
   "execution_count": 29
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Trainig the CBOW Model\n",
    "\n",
    "<img src=\"imgs/forward.png\">\n",
    "\n",
    "Now that we have implemented the necessary functions to prepare the training data and the activation functions, we can start training the CBOW model.\n",
    "\n",
    "The model will be trained using the following steps:\n",
    "1. Initialize the weights and biases.\n",
    "2. Iterate over the training examples and prepare the input and target output.\n",
    "3. Calculate the predicted output using the forward pass.\n",
    "4. Calculate the loss using the predicted output and the target output.\n",
    "5. Update the weights and biases using backpropagation.\n",
    "6. Repeat the process for a number of epochs.\n"
   ],
   "id": "40d5d5f821381013"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Exercise 9 - Initialize the weights and biases\n",
    "\n",
    "**9.1 Initialize the weights `W1` and `W2` with random values from a normal distribution using `np.random.randn` with a seed 0.**\n",
    "- `W1` should be of shape `(N, V)`.\n",
    "- `W2` should be of shape `(V, N)`.\n",
    "- `N` is the dimension of the word embedding.\n",
    "- `V` is the size of the vocabulary.\n",
    "\n",
    "**9.2 Initialize the biases `b1` and `b2` with zeros.**\n",
    "- `b1` should be of shape `(N, 1)`.\n",
    "- `b2` should be of shape `(V, 1)`.\n",
    "\n",
    "**Note:** Use the same function to initialize the weights and biases. Also, set the seed to 42.\n"
   ],
   "id": "da5d50b43257cf79"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-21T15:31:08.458457Z",
     "start_time": "2024-11-21T15:31:08.439610Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def initialize_model_parameters(V, N, random_seed=42):\n",
    "    np.random.seed(random_seed)\n",
    "    W1 = np.random.randn(N, V)\n",
    "    W2 = np.random.randn(V, N)\n",
    "    b1 = np.zeros((N, 1))\n",
    "    b2 = np.zeros((V, 1))\n",
    "    return W1, W2, b1, b2"
   ],
   "id": "457566eae93bc83",
   "outputs": [],
   "execution_count": 36
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-21T15:31:08.819630Z",
     "start_time": "2024-11-21T15:31:08.809098Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Example\n",
    "V = len(word2Ind)\n",
    "N = 10\n",
    "W1, W2, b1, b2 = initialize_model_parameters(V, N)\n",
    "print(W1.shape, W2.shape, b1.shape, b2.shape)"
   ],
   "id": "9881d2018ea4701b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 5966) (5966, 10) (10, 1) (5966, 1)\n"
     ]
    }
   ],
   "execution_count": 37
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-21T15:31:09.131814Z",
     "start_time": "2024-11-21T15:31:09.121185Z"
    }
   },
   "cell_type": "code",
   "source": [
    "assert W1.shape == (10, 5966), f\"Error: The shape of W1 should be (5841, 10) but got {W1.shape}\"\n",
    "assert W2.shape == (5966, 10), f\"Error: The shape of W2 should be (10, 5841) but got {W2.shape}\"\n",
    "assert b1.shape == (10, 1), f\"Error: The shape of b1 should be (10, 1) but got {b1.shape}\"\n",
    "assert b2.shape == (5966, 1), f\"Error: The shape of b2 should be (5841, 1) but got {b2.shape}\""
   ],
   "id": "b14524b5b558c90a",
   "outputs": [],
   "execution_count": 38
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Exercise 10 - Loss Function (Cross-Entropy)\n",
    "\n",
    "**10. Implement the cross-entropy loss function.**\n",
    "- The loss function shoud receive the predicted output `y_hat` and the target output `y`.\n",
    "- The loss function, cross-entropy, is given by:\n",
    "$$ J = - \\sum_{i=1}^{V} y_i \\log(\\hat y_i) $$\n",
    "\n",
    "Where:\n",
    "- $V$ is the size of the vocabulary.\n",
    "- $y$ is the one-hot encoded target word.\n",
    "- $\\hat y$ is the predicted output.\n"
   ],
   "id": "48b47f5340b3e922"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-21T15:35:04.004287Z",
     "start_time": "2024-11-21T15:35:03.998184Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def cross_entropy_loss(y, y_hat):\n",
    "    return -np.sum(y * np.log(y_hat))"
   ],
   "id": "51a1d5f1a7d2530b",
   "outputs": [],
   "execution_count": 39
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-21T15:35:04.427304Z",
     "start_time": "2024-11-21T15:35:04.412625Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Example\n",
    "y = np.array([1, 0, 0])\n",
    "y_hat = np.array([0.7, 0.2, 0.1])\n",
    "print(cross_entropy_loss(y, y_hat))"
   ],
   "id": "b646a044fdc8575a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.35667494393873245\n"
     ]
    }
   ],
   "execution_count": 40
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-21T15:35:05.138493Z",
     "start_time": "2024-11-21T15:35:05.106624Z"
    }
   },
   "cell_type": "code",
   "source": [
    "assert np.allclose(cross_entropy_loss(y, y_hat), 0.35667494393873245), f\"Error: The loss is not correct!\""
   ],
   "id": "c20f9f4bbe4b24f8",
   "outputs": [],
   "execution_count": 41
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Exercise 11 - Forward Pass\n",
    "\n",
    "**11. Implement the forward pass of the CBOW model.**\n",
    "- The forward pass is given by:\n",
    "$$ z_1 = W_1 \\  X + b_1 $$\n",
    "$$ h = ReLU(z_1) $$\n",
    "$$ z_2 = W_2 \\  h + b_2 $$\n",
    "$$ \\hat y = softmax(z_2) $$\n",
    "\n",
    "- The function should return the predicted output $\\hat y$.\n",
    "- The function should also return the hidden layer output $h$.\n",
    "- The input $X$ is the average of the one-hot vectors of the context words.\n",
    "- The target output $y$ is the one-hot encoded vector of the center word.\n",
    "- The weights $W_1$ and $W_2$, and the biases $b_1$ and $b_2$ are the parameters of the model."
   ],
   "id": "d269803ac1e945b9"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-21T16:17:07.358872Z",
     "start_time": "2024-11-21T16:17:07.346782Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def forward_pass(X, W1, W2, b1, b2):\n",
    "    z1 = np.dot(W1, X) + b1\n",
    "    h = ReLU(z1)\n",
    "    z2 = np.dot(W2, h) + b2\n",
    "    y_hat = softmax(z2)\n",
    "    return y_hat, h"
   ],
   "id": "4f022cd901e691ae",
   "outputs": [],
   "execution_count": 47
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-21T16:17:07.943867Z",
     "start_time": "2024-11-21T16:17:07.921039Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Example\n",
    "training_examples = get_training_example(data, 2, word2Ind, V)\n",
    "x, y = next(training_examples)"
   ],
   "id": "4561c0e1d3a2ed18",
   "outputs": [],
   "execution_count": 48
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-21T16:17:08.312154Z",
     "start_time": "2024-11-21T16:17:08.300339Z"
    }
   },
   "cell_type": "code",
   "source": [
    "x.shape = (V, 1)\n",
    "y.shape = (V, 1)\n",
    "y_hat, h = forward_pass(x, W1, W2, b1, b2)\n",
    "print(y_hat)"
   ],
   "id": "187c555721396641",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[5.84213270e-05]\n",
      " [1.93369807e-04]\n",
      " [8.42189460e-05]\n",
      " ...\n",
      " [1.79660201e-04]\n",
      " [3.94039390e-05]\n",
      " [3.11104102e-05]]\n"
     ]
    }
   ],
   "execution_count": 49
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-21T16:17:09.040489Z",
     "start_time": "2024-11-21T16:17:09.020727Z"
    }
   },
   "cell_type": "code",
   "source": [
    "assert y_hat.shape == (5966, 1), f\"Error: The shape of y_hat should be (5966, 1) but got {y_hat.shape}\""
   ],
   "id": "852f2f7e81d48eb3",
   "outputs": [],
   "execution_count": 50
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Exercise 12 - Backward Pass\n",
    "\n",
    "**12. Implement the backward pass of the CBOW model.**\n",
    "- The backward pass is used to calculate the gradients of the model parameters.\n",
    "- The gradients are used to update the weights and biases during training.\n",
    "- The function should receive the input $X$, the target output $y$, the predicted output $\\hat y$, the hidden layer output $h$, and the model parameters $W1$, $W2$, $b1$, and $b2$ and finally the learning rate $\\alpha$.\n",
    "- The function should return the updated weights and biases.\n",
    "\n",
    "The gradients are calculated as follows:\n",
    "$$ \\frac{\\partial J}{\\partial z_2} = \\hat y - y $$\n",
    "$$ \\frac{\\partial J}{\\partial W_2} = \\frac{\\partial J}{\\partial z_2} \\cdot h^T $$\n",
    "$$ \\frac{\\partial J}{\\partial b_2} = \\frac{\\partial J}{\\partial z_2} $$\n",
    "$$ \\frac{\\partial J}{\\partial z_1} = W_2^T \\cdot \\frac{\\partial J}{\\partial z_2} \\cdot ReLU'(z_1) $$\n",
    "$$ \\frac{\\partial J}{\\partial W_1} = \\frac{\\partial J}{\\partial z_1} \\cdot X^T $$\n",
    "$$ \\frac{\\partial J}{\\partial b_1} = \\frac{\\partial J}{\\partial z_1} $$\n",
    "\n",
    "The weight and bias updates are calculated using the gradients and the learning rate $\\alpha$:\n",
    "\n",
    "$$ W_1 = W_1 - \\alpha \\cdot \\frac{\\partial J}{\\partial W_1} $$\n",
    "$$ W_2 = W_2 - \\alpha \\cdot \\frac{\\partial J}{\\partial W_2} $$\n",
    "$$ b_1 = b_1 - \\alpha \\cdot \\frac{\\partial J}{\\partial b_1} $$\n",
    "$$ b_2 = b_2 - \\alpha \\cdot \\frac{\\partial J}{\\partial b_2} $$\n"
   ],
   "id": "d9965d0e947d9cf4"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-21T16:17:10.475252Z",
     "start_time": "2024-11-21T16:17:10.467050Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def backward_pass(X, y, y_hat, h, W1, W2, b1, b2, alpha=0.03):\n",
    "    dz2 = y_hat-y\n",
    "    dw2 = np.dot(dz2, h.T)\n",
    "    db2 = np.sum(dz2, axis=0, keepdims=True)\n",
    "    dz1 = np.dot(W2.T, X) * (h>0)\n",
    "    dw1 = np.dot(dz1, X.T)\n",
    "    db1 = np.sum(dz1, axis=0, keepdims=True)\n",
    "    return W1, W2, b1, b2\n",
    "    "
   ],
   "id": "27ce7683870a7fd7",
   "outputs": [],
   "execution_count": 51
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-21T16:17:10.955165Z",
     "start_time": "2024-11-21T16:17:10.926144Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Example\n",
    "V = len(word2Ind)\n",
    "N = 10\n",
    "W1, W2, b1, b2 = initialize_model_parameters(V, N)\n",
    "x, y = next(get_training_example(data, 2, word2Ind, V))\n",
    "x.shape = (V, 1)\n",
    "y.shape = (V, 1)\n",
    "y_hat, h = forward_pass(x, W1, W2, b1, b2)\n",
    "W1, W2, b1, b2 = backward_pass(x, y, y_hat, h, W1, W2, b1, b2)\n",
    "print(W1.shape, W2.shape, b1.shape, b2.shape)"
   ],
   "id": "bed6520d82043a78",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 5966) (5966, 10) (10, 1) (5966, 1)\n"
     ]
    }
   ],
   "execution_count": 52
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-21T16:17:11.600712Z",
     "start_time": "2024-11-21T16:17:11.561777Z"
    }
   },
   "cell_type": "code",
   "source": [
    "assert W1.shape == (N, V), f\"Error: The shape of W1 should be ({N}, {V}) but got {W1.shape}\"\n",
    "assert W2.shape == (V, N), f\"Error: The shape of W2 should be ({V}, {N}) but got {W2.shape}\"\n",
    "assert b1.shape == (N, 1), f\"Error: The shape of b1 should be ({N}, 1) but got {b1.shape}\"\n",
    "assert b2.shape == (V, 1), f\"Error: The shape of b2 should be ({V}, 1) but got {b2.shape}\""
   ],
   "id": "145e352e5d9f1992",
   "outputs": [],
   "execution_count": 53
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Exercise 13 - Training the CBOW Model with Gradient Descent\n",
    "\n",
    "**13. Implement the training loop of the CBOW model.**\n",
    "- The training loop should iterate over the training examples and update the weights and biases using the backward pass.\n",
    "- Compute the loss at each epoch and print it.\n",
    "- The function should return the final weights and biases.\n"
   ],
   "id": "99972f53fba92c7"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-21T16:17:12.393493Z",
     "start_time": "2024-11-21T16:17:12.383032Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def gradient_descent(data, word2Ind, N, alpha, epochs, seed=42):\n",
    "    V = len(word2Ind)\n",
    "    W1, W2, b1, b2 = initialize_model_parameters(V, N)\n",
    "    for epoch in range(epochs):\n",
    "        training_examples = get_training_example(data, 2, word2Ind, V)\n",
    "        for x, y in training_examples:\n",
    "            y_hat, h =  forward_pass(x, W1, W2, b1, b2)\n",
    "            W1, W2, b1, b2 = backward_pass(x, y, y_hat, h, W1, W2, b1, b2, alpha)\n",
    "        loss = cross_entropy_loss(y, y_hat)\n",
    "        print(f\"Epoch {epoch + 1}/{epochs}: Loss: {loss:.4f}\")\n",
    "    return W1, W2, b1, b2"
   ],
   "id": "c89eb38dcd0fcb89",
   "outputs": [],
   "execution_count": 54
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-21T16:17:12.963374Z",
     "start_time": "2024-11-21T16:17:12.904219Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Example\n",
    "N = 10\n",
    "alpha = 0.03\n",
    "epochs = 5\n",
    "W1, W2, b1, b2 = gradient_descent(data, word2Ind, N, alpha, epochs)"
   ],
   "id": "a716941ec121c595",
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (5966,10) (5966,) ",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[55], line 5\u001B[0m\n\u001B[1;32m      3\u001B[0m alpha \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0.03\u001B[39m\n\u001B[1;32m      4\u001B[0m epochs \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m5\u001B[39m\n\u001B[0;32m----> 5\u001B[0m W1, W2, b1, b2 \u001B[38;5;241m=\u001B[39m \u001B[43mgradient_descent\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdata\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mword2Ind\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mN\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43malpha\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mepochs\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[0;32mIn[54], line 8\u001B[0m, in \u001B[0;36mgradient_descent\u001B[0;34m(data, word2Ind, N, alpha, epochs, seed)\u001B[0m\n\u001B[1;32m      6\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m x, y \u001B[38;5;129;01min\u001B[39;00m training_examples:\n\u001B[1;32m      7\u001B[0m     y_hat, h \u001B[38;5;241m=\u001B[39m  forward_pass(x, W1, W2, b1, b2)\n\u001B[0;32m----> 8\u001B[0m     W1, W2, b1, b2 \u001B[38;5;241m=\u001B[39m \u001B[43mbackward_pass\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my_hat\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mh\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mW1\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mW2\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mb1\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mb2\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43malpha\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m      9\u001B[0m loss \u001B[38;5;241m=\u001B[39m cross_entropy_loss(y, y_hat)\n\u001B[1;32m     10\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mEpoch \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mepoch\u001B[38;5;250m \u001B[39m\u001B[38;5;241m+\u001B[39m\u001B[38;5;250m \u001B[39m\u001B[38;5;241m1\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m/\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mepochs\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m: Loss: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mloss\u001B[38;5;132;01m:\u001B[39;00m\u001B[38;5;124m.4f\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n",
      "Cell \u001B[0;32mIn[51], line 2\u001B[0m, in \u001B[0;36mbackward_pass\u001B[0;34m(X, y, y_hat, h, W1, W2, b1, b2, alpha)\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mbackward_pass\u001B[39m(X, y, y_hat, h, W1, W2, b1, b2, alpha\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0.03\u001B[39m):\n\u001B[0;32m----> 2\u001B[0m     dz2 \u001B[38;5;241m=\u001B[39m \u001B[43my_hat\u001B[49m\u001B[38;5;241;43m-\u001B[39;49m\u001B[43my\u001B[49m\n\u001B[1;32m      3\u001B[0m     dw2 \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mdot(dz2, h\u001B[38;5;241m.\u001B[39mT)\n\u001B[1;32m      4\u001B[0m     db2 \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39msum(dz2, axis\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0\u001B[39m, keepdims\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n",
      "\u001B[0;31mValueError\u001B[0m: operands could not be broadcast together with shapes (5966,10) (5966,) "
     ]
    }
   ],
   "execution_count": 55
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Exercise 14 - Extracting the Word Embeddings\n",
    "\n",
    "**14. Implement three versions of the function `extract_word_embedding(W1, word, word2Ind)` that returns the word embedding of a given word.**\n",
    "\n",
    "- 14.1 Extract the embeddings from the weights $W_1$.\n",
    "- 14.2 Extract the embedding from the weights $W_2$.\n",
    "- 14.3 Return the average of the two embeddings."
   ],
   "id": "7cc3fa2ab82dfa20"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-21T16:28:39.917645Z",
     "start_time": "2024-11-21T16:28:39.910176Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def extract_word_embedding_w1(W1, word, word2Ind):\n",
    "    indx = word2Ind[word]\n",
    "    return W1[:, indx]"
   ],
   "id": "ce987fcff2721e66",
   "outputs": [],
   "execution_count": 56
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-21T16:28:40.240114Z",
     "start_time": "2024-11-21T16:28:40.229368Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def extract_word_embedding_w2(W2, word, word2Ind):\n",
    "    indx = word2Ind[word]\n",
    "    return W2.T[:, indx]"
   ],
   "id": "a40523ac60093e30",
   "outputs": [],
   "execution_count": 57
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-21T16:28:40.694803Z",
     "start_time": "2024-11-21T16:28:40.654977Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def extract_word_embedding_avg(W1, W2, word, word2Ind):\n",
    "    return (extract_word_embedding_w1(W1, word, word2Ind) + extract_word_embedding_w2(W2, word, word2Ind)) / 2"
   ],
   "id": "b278955748c530af",
   "outputs": [],
   "execution_count": 58
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-21T16:28:43.827791Z",
     "start_time": "2024-11-21T16:28:43.805699Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Example\n",
    "word = 'king'\n",
    "embedding_w1 = extract_word_embedding_w1(W1, word, word2Ind)\n",
    "embedding_w2 = extract_word_embedding_w2(W2, word, word2Ind)\n",
    "embedding_avg = extract_word_embedding_avg(W1, W2, word, word2Ind)\n",
    "embedding_w1"
   ],
   "id": "5445549c60f4f5e",
   "outputs": [
    {
     "data": {
      "text/plain": "array([ 0.34115197,  0.95580275, -1.16750082, -0.37372087,  0.27064489,\n        0.14966891, -0.5179258 , -0.65786382,  0.70013765, -0.8131994 ])"
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 59
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-21T16:28:44.498566Z",
     "start_time": "2024-11-21T16:28:44.485891Z"
    }
   },
   "cell_type": "code",
   "source": [
    "embedding_w2"
   ],
   "id": "82d1f8dd7652696a",
   "outputs": [
    {
     "data": {
      "text/plain": "array([-1.23283013, -0.1548931 ,  1.27073275,  0.62265478,  0.52100579,\n        0.74733943,  0.32492521, -0.33723601,  0.96486671, -0.47583241])"
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 60
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-21T16:28:45.241998Z",
     "start_time": "2024-11-21T16:28:45.230593Z"
    }
   },
   "cell_type": "code",
   "source": [
    "embedding_avg"
   ],
   "id": "43a453578fc18ff6",
   "outputs": [
    {
     "data": {
      "text/plain": "array([-0.44583908,  0.40045482,  0.05161596,  0.12446695,  0.39582534,\n        0.44850417, -0.0965003 , -0.49754991,  0.83250218, -0.64451591])"
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 61
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-21T16:28:45.892189Z",
     "start_time": "2024-11-21T16:28:45.860416Z"
    }
   },
   "cell_type": "code",
   "source": [
    "assert embedding_w1.shape == (N,), f\"Error: The shape of the embedding should be ({N},) but got {embedding_w1.shape}\"\n",
    "assert embedding_w2.shape == (N,), f\"Error: The shape of the embedding should be ({N},) but got {embedding_w2.shape}\"\n",
    "assert embedding_avg.shape == (N,), f\"Error: The shape of the embedding should be ({N},) but got {embedding_avg.shape}\""
   ],
   "id": "81fc8adfdb88c754",
   "outputs": [],
   "execution_count": 62
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Exercise 15 - Visualizing the Word Embeddings\n",
    "\n",
    "**15. Implement a function `plot_word_embeddings(W, word2Ind, words)` that plots the word embeddings of the given words.**\n",
    "\n",
    "**Note:**\n",
    "- Use the PCA algorithm to reduce the dimensionality of the embeddings to 2D.\n",
    "- Use the `scatter` function from `matplotlib.pyplot` to plot the embeddings."
   ],
   "id": "46bf70c30fa8b7b4"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-21T16:37:03.139523Z",
     "start_time": "2024-11-21T16:37:01.235068Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_word_embeddings(W, word2Ind, words):\n",
    "    pca = PCA(n_components=2)\n",
    "    reduced = pca.fit_transform(W.T)\n",
    "    indices = [word2Ind[word] for word in words]\n",
    "    plt.scatter(reduced[indices, 0], reduced[indices, 1], c='blue')\n",
    "    for i in indices: \n",
    "        plt.text(reduced[i, 0], reduced[i, 1], Ind2word[i])\n",
    "    plt.show()"
   ],
   "id": "a22d7be3cd3572e2",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A module that was compiled using NumPy 1.x cannot be run in\n",
      "NumPy 2.0.2 as it may crash. To support both 1.x and 2.x\n",
      "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
      "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
      "\n",
      "If you are a user of the module, the easiest solution will be to\n",
      "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
      "We expect that some modules will need time to support NumPy 2.\n",
      "\n",
      "Traceback (most recent call last):  File \"/Users/tiago/anaconda3/envs/tac-hands-on/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"/Users/tiago/anaconda3/envs/tac-hands-on/lib/python3.10/runpy.py\", line 86, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"/Users/tiago/anaconda3/envs/tac-hands-on/lib/python3.10/site-packages/ipykernel_launcher.py\", line 17, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"/Users/tiago/anaconda3/envs/tac-hands-on/lib/python3.10/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\n",
      "    app.start()\n",
      "  File \"/Users/tiago/anaconda3/envs/tac-hands-on/lib/python3.10/site-packages/ipykernel/kernelapp.py\", line 701, in start\n",
      "    self.io_loop.start()\n",
      "  File \"/Users/tiago/anaconda3/envs/tac-hands-on/lib/python3.10/site-packages/tornado/platform/asyncio.py\", line 205, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"/Users/tiago/anaconda3/envs/tac-hands-on/lib/python3.10/asyncio/base_events.py\", line 603, in run_forever\n",
      "    self._run_once()\n",
      "  File \"/Users/tiago/anaconda3/envs/tac-hands-on/lib/python3.10/asyncio/base_events.py\", line 1909, in _run_once\n",
      "    handle._run()\n",
      "  File \"/Users/tiago/anaconda3/envs/tac-hands-on/lib/python3.10/asyncio/events.py\", line 80, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"/Users/tiago/anaconda3/envs/tac-hands-on/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 534, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"/Users/tiago/anaconda3/envs/tac-hands-on/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 523, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"/Users/tiago/anaconda3/envs/tac-hands-on/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 429, in dispatch_shell\n",
      "    await result\n",
      "  File \"/Users/tiago/anaconda3/envs/tac-hands-on/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 767, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"/Users/tiago/anaconda3/envs/tac-hands-on/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 429, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"/Users/tiago/anaconda3/envs/tac-hands-on/lib/python3.10/site-packages/ipykernel/zmqshell.py\", line 549, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"/Users/tiago/anaconda3/envs/tac-hands-on/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3075, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"/Users/tiago/anaconda3/envs/tac-hands-on/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3130, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"/Users/tiago/anaconda3/envs/tac-hands-on/lib/python3.10/site-packages/IPython/core/async_helpers.py\", line 128, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"/Users/tiago/anaconda3/envs/tac-hands-on/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3334, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"/Users/tiago/anaconda3/envs/tac-hands-on/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3517, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"/Users/tiago/anaconda3/envs/tac-hands-on/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3577, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/var/folders/lm/rsrn4zyx5l7_g9y1n8jbfgqh0000gn/T/ipykernel_19635/1648876772.py\", line 2, in <module>\n",
      "    import matplotlib.pyplot as plt\n",
      "  File \"/Users/tiago/anaconda3/envs/tac-hands-on/lib/python3.10/site-packages/matplotlib/__init__.py\", line 159, in <module>\n",
      "    from . import _api, _version, cbook, _docstring, rcsetup\n",
      "  File \"/Users/tiago/anaconda3/envs/tac-hands-on/lib/python3.10/site-packages/matplotlib/rcsetup.py\", line 28, in <module>\n",
      "    from matplotlib.colors import Colormap, is_color_like\n",
      "  File \"/Users/tiago/anaconda3/envs/tac-hands-on/lib/python3.10/site-packages/matplotlib/colors.py\", line 57, in <module>\n",
      "    from matplotlib import _api, _cm, cbook, scale\n",
      "  File \"/Users/tiago/anaconda3/envs/tac-hands-on/lib/python3.10/site-packages/matplotlib/scale.py\", line 22, in <module>\n",
      "    from matplotlib.ticker import (\n",
      "  File \"/Users/tiago/anaconda3/envs/tac-hands-on/lib/python3.10/site-packages/matplotlib/ticker.py\", line 144, in <module>\n",
      "    from matplotlib import transforms as mtransforms\n",
      "  File \"/Users/tiago/anaconda3/envs/tac-hands-on/lib/python3.10/site-packages/matplotlib/transforms.py\", line 49, in <module>\n",
      "    from matplotlib._path import (\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "_ARRAY_API not found",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "\u001B[0;31mAttributeError\u001B[0m: _ARRAY_API not found"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "initialization failed",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mImportError\u001B[0m                               Traceback (most recent call last)",
      "\u001B[0;31mImportError\u001B[0m: numpy.core.multiarray failed to import",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001B[0;31mImportError\u001B[0m                               Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[63], line 2\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01msklearn\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdecomposition\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m PCA\n\u001B[0;32m----> 2\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mmatplotlib\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpyplot\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mplt\u001B[39;00m\n\u001B[1;32m      4\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mplot_word_embeddings\u001B[39m(W, word2Ind, words):\n\u001B[1;32m      5\u001B[0m     pca \u001B[38;5;241m=\u001B[39m PCA(n_components\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m2\u001B[39m)\n",
      "File \u001B[0;32m~/anaconda3/envs/tac-hands-on/lib/python3.10/site-packages/matplotlib/__init__.py:159\u001B[0m\n\u001B[1;32m    155\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mpackaging\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mversion\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m parse \u001B[38;5;28;01mas\u001B[39;00m parse_version\n\u001B[1;32m    157\u001B[0m \u001B[38;5;66;03m# cbook must import matplotlib only within function\u001B[39;00m\n\u001B[1;32m    158\u001B[0m \u001B[38;5;66;03m# definitions, so it is safe to import from it here.\u001B[39;00m\n\u001B[0;32m--> 159\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m _api, _version, cbook, _docstring, rcsetup\n\u001B[1;32m    160\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mmatplotlib\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mcbook\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m sanitize_sequence\n\u001B[1;32m    161\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mmatplotlib\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_api\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m MatplotlibDeprecationWarning\n",
      "File \u001B[0;32m~/anaconda3/envs/tac-hands-on/lib/python3.10/site-packages/matplotlib/rcsetup.py:28\u001B[0m\n\u001B[1;32m     26\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mmatplotlib\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mbackends\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m BackendFilter, backend_registry\n\u001B[1;32m     27\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mmatplotlib\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mcbook\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m ls_mapper\n\u001B[0;32m---> 28\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mmatplotlib\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mcolors\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Colormap, is_color_like\n\u001B[1;32m     29\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mmatplotlib\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_fontconfig_pattern\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m parse_fontconfig_pattern\n\u001B[1;32m     30\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mmatplotlib\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_enums\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m JoinStyle, CapStyle\n",
      "File \u001B[0;32m~/anaconda3/envs/tac-hands-on/lib/python3.10/site-packages/matplotlib/colors.py:57\u001B[0m\n\u001B[1;32m     55\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mmatplotlib\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mmpl\u001B[39;00m\n\u001B[1;32m     56\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mnumpy\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mnp\u001B[39;00m\n\u001B[0;32m---> 57\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mmatplotlib\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m _api, _cm, cbook, scale\n\u001B[1;32m     58\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_color_data\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m BASE_COLORS, TABLEAU_COLORS, CSS4_COLORS, XKCD_COLORS\n\u001B[1;32m     61\u001B[0m \u001B[38;5;28;01mclass\u001B[39;00m \u001B[38;5;21;01m_ColorMapping\u001B[39;00m(\u001B[38;5;28mdict\u001B[39m):\n",
      "File \u001B[0;32m~/anaconda3/envs/tac-hands-on/lib/python3.10/site-packages/matplotlib/scale.py:22\u001B[0m\n\u001B[1;32m     20\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mmatplotlib\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mmpl\u001B[39;00m\n\u001B[1;32m     21\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mmatplotlib\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m _api, _docstring\n\u001B[0;32m---> 22\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mmatplotlib\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mticker\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m (\n\u001B[1;32m     23\u001B[0m     NullFormatter, ScalarFormatter, LogFormatterSciNotation, LogitFormatter,\n\u001B[1;32m     24\u001B[0m     NullLocator, LogLocator, AutoLocator, AutoMinorLocator,\n\u001B[1;32m     25\u001B[0m     SymmetricalLogLocator, AsinhLocator, LogitLocator)\n\u001B[1;32m     26\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mmatplotlib\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mtransforms\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Transform, IdentityTransform\n\u001B[1;32m     29\u001B[0m \u001B[38;5;28;01mclass\u001B[39;00m \u001B[38;5;21;01mScaleBase\u001B[39;00m:\n",
      "File \u001B[0;32m~/anaconda3/envs/tac-hands-on/lib/python3.10/site-packages/matplotlib/ticker.py:144\u001B[0m\n\u001B[1;32m    142\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mmatplotlib\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mmpl\u001B[39;00m\n\u001B[1;32m    143\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mmatplotlib\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m _api, cbook\n\u001B[0;32m--> 144\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mmatplotlib\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m transforms \u001B[38;5;28;01mas\u001B[39;00m mtransforms\n\u001B[1;32m    146\u001B[0m _log \u001B[38;5;241m=\u001B[39m logging\u001B[38;5;241m.\u001B[39mgetLogger(\u001B[38;5;18m__name__\u001B[39m)\n\u001B[1;32m    148\u001B[0m __all__ \u001B[38;5;241m=\u001B[39m (\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mTickHelper\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mFormatter\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mFixedFormatter\u001B[39m\u001B[38;5;124m'\u001B[39m,\n\u001B[1;32m    149\u001B[0m            \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mNullFormatter\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mFuncFormatter\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mFormatStrFormatter\u001B[39m\u001B[38;5;124m'\u001B[39m,\n\u001B[1;32m    150\u001B[0m            \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mStrMethodFormatter\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mScalarFormatter\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mLogFormatter\u001B[39m\u001B[38;5;124m'\u001B[39m,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    156\u001B[0m            \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mMultipleLocator\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mMaxNLocator\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mAutoMinorLocator\u001B[39m\u001B[38;5;124m'\u001B[39m,\n\u001B[1;32m    157\u001B[0m            \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mSymmetricalLogLocator\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mAsinhLocator\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mLogitLocator\u001B[39m\u001B[38;5;124m'\u001B[39m)\n",
      "File \u001B[0;32m~/anaconda3/envs/tac-hands-on/lib/python3.10/site-packages/matplotlib/transforms.py:49\u001B[0m\n\u001B[1;32m     46\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mnumpy\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mlinalg\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m inv\n\u001B[1;32m     48\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mmatplotlib\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m _api\n\u001B[0;32m---> 49\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mmatplotlib\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_path\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m (\n\u001B[1;32m     50\u001B[0m     affine_transform, count_bboxes_overlapping_bbox, update_path_extents)\n\u001B[1;32m     51\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpath\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Path\n\u001B[1;32m     53\u001B[0m DEBUG \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m\n",
      "\u001B[0;31mImportError\u001B[0m: initialization failed"
     ]
    }
   ],
   "execution_count": 63
  },
  {
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-11-21T15:05:04.121001Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Example\n",
    "words = ['king', 'queen', 'man', 'woman']\n",
    "plot_word_embeddings(W1, word2Ind, words)"
   ],
   "id": "476e153529904a9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-11-21T15:05:04.122998Z"
    }
   },
   "cell_type": "code",
   "source": [
    "plot_word_embeddings(W2.T, word2Ind, words)"
   ],
   "id": "3a9842757114ca48",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-11-21T15:05:04.124766Z"
    }
   },
   "cell_type": "code",
   "source": [
    "plot_word_embeddings((W1 + W2.T) / 2, word2Ind, words)"
   ],
   "id": "5b61950a3ff20886",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-11-21T15:05:04.126375Z"
    }
   },
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [],
   "id": "18cd8f38c94f8c70"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
